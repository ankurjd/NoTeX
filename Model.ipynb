{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"C:/Users/aiden/Downloads/ProjectData/FeatureData/\"\n",
    "#with open(datapath+\"Train\", \"rb\") as f:\n",
    "#    X_train, Y_train, _ = pkl.load(f)\n",
    "with open(datapath+\"Test\", \"rb\") as f:\n",
    "    X_test, Y_test, _ = pkl.load(f)\n",
    "with open(\"C:/Users/aiden/Downloads/ProjectData/Vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=tf.ragged.constant(X_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DynamicRaggedShape lengths=[10, (43, 11, 39, 11, 13, 37, 59, 15, 15, 13), (7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7)] num_row_partitions=2>\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(X_train_tensor[:10,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_index (data):\n",
    "    return [[vocab[w] for w in arr] for arr in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_VOCAB_SIZE=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CELoss(y_pred, x):\n",
    "    on_stroke_indicator=x[:,:,6]\n",
    "    #assumes that y_pred is log probabilities\n",
    "    #log_prob_of_symbol = tf.math.reduce_logsumexp(y_pred[:,:,RELATION_VOCAB_SIZE:], axis=2)\n",
    "    prob_of_symbol = tf.math.reduce_sum(y_pred[:,:,RELATION_VOCAB_SIZE:], axis=2)\n",
    "    return(tf.math.reduce_sum(on_stroke_indicator*(-tf.math.log(prob_of_symbol))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.12857143 0.12857143 0.12857143 0.12857143 0.12857143 0.12857143\n",
      "   0.12857143 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957]\n",
      "  [0.12857143 0.12857143 0.12857143 0.12857143 0.12857143 0.12857143\n",
      "   0.12857143 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957]\n",
      "  [0.12857143 0.12857143 0.12857143 0.12857143 0.12857143 0.12857143\n",
      "   0.12857143 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957]\n",
      "  [0.12857143 0.12857143 0.12857143 0.12857143 0.12857143 0.12857143\n",
      "   0.12857143 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957]\n",
      "  [0.12857143 0.12857143 0.12857143 0.12857143 0.12857143 0.12857143\n",
      "   0.12857143 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957 0.00086957 0.00086957 0.00086957 0.00086957\n",
      "   0.00086957 0.00086957]]]\n"
     ]
    }
   ],
   "source": [
    "y_offstroke_vector=np.zeros(vocab_size)\n",
    "y_offstroke_vector[RELATION_VOCAB_SIZE:]=0.1/(vocab_size-RELATION_VOCAB_SIZE)\n",
    "y_offstroke_vector[:RELATION_VOCAB_SIZE]=0.9/RELATION_VOCAB_SIZE\n",
    "y_onstroke_vector=np.zeros(vocab_size)\n",
    "y_onstroke_vector[RELATION_VOCAB_SIZE:]=0.9/(vocab_size-RELATION_VOCAB_SIZE)\n",
    "y_onstroke_vector[:RELATION_VOCAB_SIZE]=0.1/RELATION_VOCAB_SIZE\n",
    "\n",
    "def get_y_pred(x, y_offstroke_vector, y_onstroke_vector):\n",
    "    #x input shape = (batch_size, sequence_len, 7)\n",
    "    y = np.zeros((x.shape[0], x.shape[1], y_offstroke_vector.shape[0]))\n",
    "    y[x[:,:,6] == 1] =  y_onstroke_vector\n",
    "    print(x[:,:,6]==1)\n",
    "    y[x[:,:,6] == 0] = y_offstroke_vector\n",
    "    return y\n",
    "#CELoss Test\n",
    "x=np.zeros((1, 5, 7))\n",
    "x[:,:,6]=[1,0,1,0,1]\n",
    "print(get_y_pred(x, y_offstroke_vector, y_offstroke_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MathModel(input_shape, output_dim):\n",
    "    input = Input(shape=input_shape)\n",
    "    X = Bidirectional(LSTM(128, return_sequences = True))(input)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Bidirectional(LSTM(128, return_sequences = True))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Bidirectional(LSTM(128, return_sequences = True))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(output_dim+1)(X)\n",
    "    output = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=input,outputs=output)\n",
    "    model.add_loss(CELoss(output,input))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCLoss(y_true, y_pred):\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_y = data_to_index(Y_train)\n",
    "indexed_y=tf.ragged.constant(indexed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'Sub',\n",
       " 'n',\n",
       " 'NoRel',\n",
       " '(',\n",
       " 'Right',\n",
       " '1',\n",
       " 'Right',\n",
       " '-',\n",
       " 'Right',\n",
       " 'x',\n",
       " 'Right',\n",
       " ')',\n",
       " 'Right',\n",
       " '=',\n",
       " 'Right',\n",
       " '(',\n",
       " 'Right',\n",
       " '-',\n",
       " 'Right',\n",
       " '1',\n",
       " 'Right',\n",
       " ')',\n",
       " 'Sup',\n",
       " 'n',\n",
       " 'NoRel',\n",
       " 'B',\n",
       " 'Sub',\n",
       " 'n',\n",
       " 'NoRel',\n",
       " '(',\n",
       " 'Right',\n",
       " 'x',\n",
       " 'Right',\n",
       " ')']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class MathDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch_size, X_train_data_len, Y_train_data_len):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.batch_size=batch_size\n",
    "        self.X_train_data_len=X_train_data_len\n",
    "        self.Y_train_data_len=Y_train_data_len\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        max_x=max(self.X_train_data_len[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size])\n",
    "        max_y=max(self.Y_train_data_len[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size])\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size, :max_x]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size, :max_y]\n",
    "\n",
    "        return (batch_x,batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, trainData, learning_rate, momentum, epochs):\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum), loss=CTCLoss)\n",
    "    model.fit(trainData,epochs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MathModel(input_shape=(None,7), output_dim=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = CTCLoss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"tf.__operators__.getitem_2\" \"                 f\"(type SlicingOpLambda).\n    \n    Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:2\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n    \n    Call arguments received by layer \"tf.__operators__.getitem_2\" \"                 f\"(type SlicingOpLambda):\n      • tensor=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:2\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64))\n      • slice_spec=({'start': 'None', 'stop': 'None', 'step': 'None'}, {'start': 'None', 'stop': 'None', 'step': 'None'}, '6')\n      • var=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_model\u001b[39m=\u001b[39mtrainModel(model, X_train_tensor, indexed_y, \u001b[39m0.1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, X, Y, learning_rate, momentum, batch_size, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrainModel\u001b[39m(model, X, Y, learning_rate, momentum, batch_size, epochs):\n\u001b[0;32m      2\u001b[0m     model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(learning_rate\u001b[39m=\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39mmomentum), loss\u001b[39m=\u001b[39mCTCLoss)\n\u001b[1;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X,Y,batch_size,epochs)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filek5813fzn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\aiden\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"tf.__operators__.getitem_2\" \"                 f\"(type SlicingOpLambda).\n    \n    Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:2\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n    \n    Call arguments received by layer \"tf.__operators__.getitem_2\" \"                 f\"(type SlicingOpLambda):\n      • tensor=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:2\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64))\n      • slice_spec=({'start': 'None', 'stop': 'None', 'step': 'None'}, {'start': 'None', 'stop': 'None', 'step': 'None'}, '6')\n      • var=None\n"
     ]
    }
   ],
   "source": [
    "new_model=trainModel(model, X_train_tensor, indexed_y, 0.1, 0, 32, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, None, 7)]    0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, None, 256)   139264      ['input_5[0][0]']                \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, None, 256)    0           ['bidirectional_11[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_12 (Bidirectiona  (None, None, 256)   394240      ['dropout_10[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, None, 256)    0           ['bidirectional_12[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_13 (Bidirectiona  (None, None, 256)   394240      ['dropout_11[0][0]']             \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, None, 256)    0           ['bidirectional_13[0][0]']       \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, None, 111)    28527       ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, None, 111)    0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, None, 104)   0           ['activation_3[0][0]']           \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, None)        0           ['tf.__operators__.getitem_3[0][0\n",
      " )                                                               ]']                              \n",
      "                                                                                                  \n",
      " tf.math.log (TFOpLambda)       (None, None)         0           ['tf.math.reduce_sum[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, None)        0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.negative (TFOpLambda)  (None, None)         0           ['tf.math.log[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, None)         0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.negative[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  ()                  0           ['tf.math.multiply[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " add_loss (AddLoss)             ()                   0           ['tf.math.reduce_sum_1[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 956,271\n",
      "Trainable params: 956,271\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6737d4ed9008f34a36365f9afa168c058c7bcfc27ae9ebfad5913b219a0345d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
